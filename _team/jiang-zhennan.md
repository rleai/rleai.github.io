---
layout: profile
title: Jiang, Zhennan
name: Jiang, Zhennan
role: Ph.D.
image: /assets/img/team/jiangzhennan.jpg
permalink: /team/jiang-zhennan/
email: stjzn0410@gmail.com
education:
  - degree: Ph.D. Candidate
    institution:  Zhongguancun Academy & Insitute of Automation, Chinese Academy of Sciences
    period: 2024-Present
    advisor: Prof. Yu Chao & Prof. Zhao Dongbin & Prof. Li Haoran
    major: Computer Science
  - degree: B.Sc. 
    institution: Central South University
    period: 2020-2024
    major: Automation
    rank: 1/240

research_areas:
  - Deep Reinforcement Learning
  - Embodied AI
  - Deep Generative Model

biography: |
  Welcome! I’m Jiang Zhennan (江震南), a first-year Ph.D. student in Technology for Computer Applications at the Institute of Automation, Chinese Academy of Sciences (CASIA). I am fortunate to have Prof. Zhao Dongbin (IEEE Fellow) as my chief supervisor, alongside Dr. Li Haoran as my co-supervisor. Currently, I am also undergoing joint training at Zhongguancun Academy, under the mentorship of Prof. Yu Chao.

  My research interests currently focus on reinforcement learning and robotics. In the future, I aim to delve into embodied intelligence technologies and their applications.

publications:
  - title: "Generalizing Consistency Policy to Visual RL with Prioritized Proximal Experience Regularization"
    authors: "Haoran Li, Zhennan Jiang, Yuhui Chen, and Dongbin Zhao"
    venue: "NeurIPS 2024"
    year: 2024
    abstract: "With high-dimensional state spaces, visual reinforcement learning (RL) faces significant challenges in exploitation and exploration, resulting in low sample efficiency and training stability. As a time-efficient diffusion model, although consistency models have been validated in online state-based RL, it is still an open question whether it can be extended to visual RL. In this paper, we investigate the impact of non-stationary distribution and the actor-critic framework on consistency policy in online RL, and find that consistency policy was unstable during the training, especially in visual RL with the high-dimensional state space. To this end, we suggest sample-based entropy regularization to stabilize the policy training, and propose a consistency policy with prioritized proximal experience regularization (CP3ER) to improve sample efficiency. CP3ER achieves new state-of-the-art (SOTA) performance in 21 tasks across DeepMind control suite and Meta-world. To our knowledge, CP3ER is the first method to apply diffusion/consistency models to visual RL and demonstrates the potential of consistency models in visual RL."
    citation: "Li, H., Jiang, Z., Chen, Y., & Zhao, D. (2024). Generalizing consistency policy to visual RL with prioritized proximal experience regularization. In Proceedings of the Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS 2024)."

social:
  - platform: Google Scholar
    url: https://scholar.google.com/citations?user=-AHCSxIAAAAJ&hl=en
    icon: fas fa-graduation-cap
  - platform: GitHub
    url: https://github.com/jzndd
    icon: fab fa-github
  - platform: LinkedIn
    url: https://www.linkedin.com/in/%E9%9C%87%E5%8D%97-%E6%B1%9F-012809309/
    icon: fab fa-linkedin

contact:
  email: stjzn0410@gmail.com
  github: https://github.com/jzndd
  linkedin: https://www.linkedin.com/in/%E9%9C%87%E5%8D%97-%E6%B1%9F-012809309/
  google_scholar: https://scholar.google.com/citations?user=-AHCSxIAAAAJ&hl=en
--- 